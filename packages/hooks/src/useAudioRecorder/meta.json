{
  "$schema": "https://raw.githubusercontent.com/sseuniverse/sse-hooks/refs/heads/main/schema/meta.json",
  "name": "use-audio-recorder",
  "type": "registry:hook",
  "title": "useAudioRecorder",
  "description": "Supported audio MIME types for recording.",
  "dependencies": [
    "react"
  ],
  "registryDependencies": [
    "with-defaults"
  ],
  "file": {
    "content": "type AudioMimeType =\r\n  | \"audio/webm\"\r\n  | \"audio/webm;codecs=opus\"\r\n  | \"audio/webm;codecs=vorbis\"\r\n  | \"audio/ogg\"\r\n  | \"audio/ogg;codecs=opus\"\r\n  | \"audio/ogg;codecs=vorbis\"\r\n  | \"application/ogg\"\r\n  | \"audio/mp4\"\r\n  | \"audio/mp4;codecs=mp4a.40.2\"\r\n  | \"audio/aac\"\r\n  | \"audio/x-m4a\"\r\n  | \"audio/mpeg\"\r\n  | \"audio/mp3\"\r\n  | \"audio/wav\"\r\n  | \"audio/x-wav\"\r\n  | \"audio/wave\"\r\n  | \"audio/flac\"\r\n  | \"audio/3gpp\"\r\n  | \"audio/3gpp2\";\r\n\r\n\r\nexport interface UseAudioRecorderOptions {\r\n  \r\n  audioBitsPerSecond?: number;\r\n  \r\n  mimeType?: AudioMimeType;\r\n  \r\n  timeslice?: number;\r\n  \r\n  enableAnalysis?: boolean;\r\n  \r\n  fftSize?: number;\r\n}\r\n\r\n\r\nexport interface AudioAnalysisData {\r\n  \r\n  frequencyData: Uint8Array;\r\n  \r\n  timeData: Uint8Array;\r\n  \r\n  volume: number;\r\n}\r\n\r\n\r\nexport interface UseAudioRecorderReturn {\r\n  \r\n  isSupported: boolean;\r\n  \r\n  isRecording: boolean;\r\n  \r\n  isPaused: boolean;\r\n  \r\n  stream: MediaStream | null;\r\n  \r\n  mediaRecorder: MediaRecorder | null;\r\n  \r\n  audioBlob: Blob | null;\r\n  \r\n  audioUrl: string | null;\r\n  \r\n  duration: number;\r\n  \r\n  error: string | null;\r\n  \r\n  analysisData: AudioAnalysisData | null;\r\n  \r\n  startRecording: () => Promise<void>;\r\n  \r\n  stopRecording: () => void;\r\n  \r\n  pauseRecording: () => void;\r\n  \r\n  resumeRecording: () => void;\r\n  \r\n  clearRecording: () => void;\r\n  \r\n  downloadRecording: (filename?: string) => void;\r\n}\r\n\nimport { useState, useEffect, useRef, useCallback } from \"react\";\r\n\n\n\r\nexport const useAudioRecorder = (\r\n  options: UseAudioRecorderOptions = {},\r\n): UseAudioRecorderReturn => {\r\n  const { audioBitsPerSecond, mimeType, timeslice, enableAnalysis, fftSize } =\r\n    withDefaults<UseAudioRecorderOptions>(options, {\r\n      audioBitsPerSecond: 128000,\r\n      mimeType: \"audio/webm\",\r\n      enableAnalysis: false,\r\n      fftSize: 2048,\r\n    });\r\n\r\n  const [isRecording, setIsRecording] = useState(false);\r\n  const [isPaused, setIsPaused] = useState(false);\r\n  const [stream, setStream] = useState<MediaStream | null>(null);\r\n  const [mediaRecorder, setMediaRecorder] = useState<MediaRecorder | null>(\r\n    null,\r\n  );\r\n  const [audioBlob, setAudioBlob] = useState<Blob | null>(null);\r\n  const [audioUrl, setAudioUrl] = useState<string | null>(null);\r\n  const [duration, setDuration] = useState(0);\r\n  const [error, setError] = useState<string | null>(null);\r\n  const [analysisData, setAnalysisData] = useState<AudioAnalysisData | null>(\r\n    null,\r\n  );\r\n\r\n  const chunksRef = useRef<Blob[]>([]);\r\n  const startTimeRef = useRef<number>(0);\r\n  const pausedTimeRef = useRef<number>(0);\r\n  const intervalRef = useRef<NodeJS.Timeout | null>(null);\r\n  const audioContextRef = useRef<AudioContext | null>(null);\r\n  const analyserRef = useRef<AnalyserNode | null>(null);\r\n  const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null);\r\n  const animationFrameRef = useRef<number | null>(null);\r\n\r\n  const isSupported =\r\n    typeof navigator !== \"undefined\" &&\r\n    !!navigator.mediaDevices &&\r\n    !!navigator.mediaDevices.getUserMedia &&\r\n    !!window.MediaRecorder;\r\n\r\n  const updateDuration = useCallback(() => {\r\n    if (startTimeRef.current) {\r\n      const elapsed = Date.now() - startTimeRef.current - pausedTimeRef.current;\r\n      setDuration(Math.floor(elapsed / 1000));\r\n    }\r\n  }, []);\r\n\r\n  const analyzeAudio = useCallback(() => {\r\n    if (!analyserRef.current || !enableAnalysis) return;\r\n\r\n    const frequencyData = new Uint8Array(analyserRef.current.frequencyBinCount);\r\n    const timeData = new Uint8Array(analyserRef.current.fftSize);\r\n\r\n    analyserRef.current.getByteFrequencyData(frequencyData);\r\n    analyserRef.current.getByteTimeDomainData(timeData);\r\n\r\n    \r\n    let sum = 0;\r\n    for (let i = 0; i < timeData.length; i++) {\r\n      const sample = ((timeData[i] ?? 0) - 128) / 128;\r\n      sum += sample * sample;\r\n    }\r\n    const volume = Math.sqrt(sum / timeData.length);\r\n\r\n    setAnalysisData({\r\n      frequencyData: frequencyData.slice(),\r\n      timeData: timeData.slice(),\r\n      volume,\r\n    });\r\n\r\n    if (isRecording && !isPaused) {\r\n      animationFrameRef.current = requestAnimationFrame(analyzeAudio);\r\n    }\r\n  }, [isRecording, isPaused, enableAnalysis]);\r\n\r\n  const setupAudioAnalysis = useCallback(\r\n    (mediaStream: MediaStream) => {\r\n      if (!enableAnalysis) return;\r\n\r\n      try {\r\n        audioContextRef.current = new (\r\n          window.AudioContext || (window as any).webkitAudioContext\r\n        )();\r\n        analyserRef.current = audioContextRef.current.createAnalyser();\r\n        sourceRef.current =\r\n          audioContextRef.current.createMediaStreamSource(mediaStream);\r\n\r\n        analyserRef.current.fftSize = fftSize;\r\n        analyserRef.current.smoothingTimeConstant = 0.8;\r\n\r\n        sourceRef.current.connect(analyserRef.current);\r\n\r\n        analyzeAudio();\r\n      } catch (err) {\r\n        console.warn(\"Failed to setup audio analysis:\", err);\r\n      }\r\n    },\r\n    [enableAnalysis, fftSize, analyzeAudio],\r\n  );\r\n\r\n  const startRecording = useCallback(async () => {\r\n    if (!isSupported) {\r\n      setError(\"Audio recording is not supported in this browser\");\r\n      return;\r\n    }\r\n\r\n    try {\r\n      setError(null);\r\n      const mediaStream = await navigator.mediaDevices.getUserMedia({\r\n        audio: {\r\n          echoCancellation: true,\r\n          noiseSuppression: true,\r\n          autoGainControl: true,\r\n        },\r\n      });\r\n\r\n      setStream(mediaStream);\r\n      setupAudioAnalysis(mediaStream);\r\n\r\n      const recorder = new MediaRecorder(mediaStream, {\r\n        audioBitsPerSecond,\r\n        mimeType: MediaRecorder.isTypeSupported(mimeType)\r\n          ? mimeType\r\n          : \"audio/webm\",\r\n      });\r\n\r\n      chunksRef.current = [];\r\n\r\n      recorder.ondataavailable = (event) => {\r\n        if (event.data.size > 0) {\r\n          chunksRef.current.push(event.data);\r\n        }\r\n      };\r\n\r\n      recorder.onstop = () => {\r\n        const blob = new Blob(chunksRef.current, { type: recorder.mimeType });\r\n        setAudioBlob(blob);\r\n        setAudioUrl(URL.createObjectURL(blob));\r\n        setIsRecording(false);\r\n        setIsPaused(false);\r\n\r\n        if (intervalRef.current) {\r\n          clearInterval(intervalRef.current);\r\n          intervalRef.current = null;\r\n        }\r\n\r\n        if (animationFrameRef.current) {\r\n          cancelAnimationFrame(animationFrameRef.current);\r\n          animationFrameRef.current = null;\r\n        }\r\n      };\r\n\r\n      recorder.onpause = () => {\r\n        setIsPaused(true);\r\n        pausedTimeRef.current += Date.now() - startTimeRef.current;\r\n        if (animationFrameRef.current) {\r\n          cancelAnimationFrame(animationFrameRef.current);\r\n          animationFrameRef.current = null;\r\n        }\r\n      };\r\n\r\n      recorder.onresume = () => {\r\n        setIsPaused(false);\r\n        startTimeRef.current = Date.now();\r\n        if (enableAnalysis) {\r\n          analyzeAudio();\r\n        }\r\n      };\r\n\r\n      recorder.onerror = (event) => {\r\n        setError(`Recording error: ${event.error?.message || \"Unknown error\"}`);\r\n        setIsRecording(false);\r\n        setIsPaused(false);\r\n      };\r\n\r\n      setMediaRecorder(recorder);\r\n      recorder.start(timeslice);\r\n      setIsRecording(true);\r\n      startTimeRef.current = Date.now();\r\n      pausedTimeRef.current = 0;\r\n      setDuration(0);\r\n\r\n      intervalRef.current = setInterval(updateDuration, 1000);\r\n    } catch (err) {\r\n      const errorMessage =\r\n        err instanceof Error ? err.message : \"Failed to start recording\";\r\n      setError(errorMessage);\r\n    }\r\n  }, [\r\n    isSupported,\r\n    audioBitsPerSecond,\r\n    mimeType,\r\n    timeslice,\r\n    setupAudioAnalysis,\r\n    updateDuration,\r\n    enableAnalysis,\r\n    analyzeAudio,\r\n  ]);\r\n\r\n  const stopRecording = useCallback(() => {\r\n    if (mediaRecorder && mediaRecorder.state !== \"inactive\") {\r\n      mediaRecorder.stop();\r\n    }\r\n\r\n    if (stream) {\r\n      stream.getTracks().forEach((track) => track.stop());\r\n      setStream(null);\r\n    }\r\n\r\n    if (audioContextRef.current) {\r\n      audioContextRef.current.close();\r\n      audioContextRef.current = null;\r\n    }\r\n  }, [mediaRecorder, stream]);\r\n\r\n  const pauseRecording = useCallback(() => {\r\n    if (mediaRecorder && mediaRecorder.state === \"recording\") {\r\n      mediaRecorder.pause();\r\n    }\r\n  }, [mediaRecorder]);\r\n\r\n  const resumeRecording = useCallback(() => {\r\n    if (mediaRecorder && mediaRecorder.state === \"paused\") {\r\n      mediaRecorder.resume();\r\n    }\r\n  }, [mediaRecorder]);\r\n\r\n  const clearRecording = useCallback(() => {\r\n    if (audioUrl) {\r\n      URL.revokeObjectURL(audioUrl);\r\n    }\r\n    setAudioBlob(null);\r\n    setAudioUrl(null);\r\n    setDuration(0);\r\n    setAnalysisData(null);\r\n    setError(null);\r\n  }, [audioUrl]);\r\n\r\n  const downloadRecording = useCallback(\r\n    (filename = \"recording.webm\") => {\r\n      if (!audioUrl) return;\r\n\r\n      const link = document.createElement(\"a\");\r\n      link.href = audioUrl;\r\n      link.download = filename;\r\n      document.body.appendChild(link);\r\n      link.click();\r\n      document.body.removeChild(link);\r\n    },\r\n    [audioUrl],\r\n  );\r\n\r\n  \r\n  useEffect(() => {\r\n    return () => {\r\n      if (intervalRef.current) {\r\n        clearInterval(intervalRef.current);\r\n      }\r\n      if (animationFrameRef.current) {\r\n        cancelAnimationFrame(animationFrameRef.current);\r\n      }\r\n      if (stream) {\r\n        stream.getTracks().forEach((track) => track.stop());\r\n      }\r\n      if (audioContextRef.current) {\r\n        audioContextRef.current.close();\r\n      }\r\n      if (audioUrl) {\r\n        URL.revokeObjectURL(audioUrl);\r\n      }\r\n    };\r\n  }, [stream, audioUrl]);\r\n\r\n  return {\r\n    isSupported,\r\n    isRecording,\r\n    isPaused,\r\n    stream,\r\n    mediaRecorder,\r\n    audioBlob,\r\n    audioUrl,\r\n    duration,\r\n    error,\r\n    analysisData,\r\n    startRecording,\r\n    stopRecording,\r\n    pauseRecording,\r\n    resumeRecording,\r\n    clearRecording,\r\n    downloadRecording,\r\n  };\r\n};",
    "js": "import { useState, useEffect, useRef, useCallback } from \"react\";\nexport const useAudioRecorder = (options = {}) => {\n    const { audioBitsPerSecond, mimeType, timeslice, enableAnalysis, fftSize } = withDefaults(options, {\n        audioBitsPerSecond: 128000,\n        mimeType: \"audio/webm\",\n        enableAnalysis: false,\n        fftSize: 2048,\n    });\n    const [isRecording, setIsRecording] = useState(false);\n    const [isPaused, setIsPaused] = useState(false);\n    const [stream, setStream] = useState(null);\n    const [mediaRecorder, setMediaRecorder] = useState(null);\n    const [audioBlob, setAudioBlob] = useState(null);\n    const [audioUrl, setAudioUrl] = useState(null);\n    const [duration, setDuration] = useState(0);\n    const [error, setError] = useState(null);\n    const [analysisData, setAnalysisData] = useState(null);\n    const chunksRef = useRef([]);\n    const startTimeRef = useRef(0);\n    const pausedTimeRef = useRef(0);\n    const intervalRef = useRef(null);\n    const audioContextRef = useRef(null);\n    const analyserRef = useRef(null);\n    const sourceRef = useRef(null);\n    const animationFrameRef = useRef(null);\n    const isSupported = typeof navigator !== \"undefined\" &&\n        !!navigator.mediaDevices &&\n        !!navigator.mediaDevices.getUserMedia &&\n        !!window.MediaRecorder;\n    const updateDuration = useCallback(() => {\n        if (startTimeRef.current) {\n            const elapsed = Date.now() - startTimeRef.current - pausedTimeRef.current;\n            setDuration(Math.floor(elapsed / 1000));\n        }\n    }, []);\n    const analyzeAudio = useCallback(() => {\n        if (!analyserRef.current || !enableAnalysis)\n            return;\n        const frequencyData = new Uint8Array(analyserRef.current.frequencyBinCount);\n        const timeData = new Uint8Array(analyserRef.current.fftSize);\n        analyserRef.current.getByteFrequencyData(frequencyData);\n        analyserRef.current.getByteTimeDomainData(timeData);\n        let sum = 0;\n        for (let i = 0; i < timeData.length; i++) {\n            const sample = ((timeData[i] ?? 0) - 128) / 128;\n            sum += sample * sample;\n        }\n        const volume = Math.sqrt(sum / timeData.length);\n        setAnalysisData({\n            frequencyData: frequencyData.slice(),\n            timeData: timeData.slice(),\n            volume,\n        });\n        if (isRecording && !isPaused) {\n            animationFrameRef.current = requestAnimationFrame(analyzeAudio);\n        }\n    }, [isRecording, isPaused, enableAnalysis]);\n    const setupAudioAnalysis = useCallback((mediaStream) => {\n        if (!enableAnalysis)\n            return;\n        try {\n            audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();\n            analyserRef.current = audioContextRef.current.createAnalyser();\n            sourceRef.current =\n                audioContextRef.current.createMediaStreamSource(mediaStream);\n            analyserRef.current.fftSize = fftSize;\n            analyserRef.current.smoothingTimeConstant = 0.8;\n            sourceRef.current.connect(analyserRef.current);\n            analyzeAudio();\n        }\n        catch (err) {\n            console.warn(\"Failed to setup audio analysis:\", err);\n        }\n    }, [enableAnalysis, fftSize, analyzeAudio]);\n    const startRecording = useCallback(async () => {\n        if (!isSupported) {\n            setError(\"Audio recording is not supported in this browser\");\n            return;\n        }\n        try {\n            setError(null);\n            const mediaStream = await navigator.mediaDevices.getUserMedia({\n                audio: {\n                    echoCancellation: true,\n                    noiseSuppression: true,\n                    autoGainControl: true,\n                },\n            });\n            setStream(mediaStream);\n            setupAudioAnalysis(mediaStream);\n            const recorder = new MediaRecorder(mediaStream, {\n                audioBitsPerSecond,\n                mimeType: MediaRecorder.isTypeSupported(mimeType)\n                    ? mimeType\n                    : \"audio/webm\",\n            });\n            chunksRef.current = [];\n            recorder.ondataavailable = (event) => {\n                if (event.data.size > 0) {\n                    chunksRef.current.push(event.data);\n                }\n            };\n            recorder.onstop = () => {\n                const blob = new Blob(chunksRef.current, { type: recorder.mimeType });\n                setAudioBlob(blob);\n                setAudioUrl(URL.createObjectURL(blob));\n                setIsRecording(false);\n                setIsPaused(false);\n                if (intervalRef.current) {\n                    clearInterval(intervalRef.current);\n                    intervalRef.current = null;\n                }\n                if (animationFrameRef.current) {\n                    cancelAnimationFrame(animationFrameRef.current);\n                    animationFrameRef.current = null;\n                }\n            };\n            recorder.onpause = () => {\n                setIsPaused(true);\n                pausedTimeRef.current += Date.now() - startTimeRef.current;\n                if (animationFrameRef.current) {\n                    cancelAnimationFrame(animationFrameRef.current);\n                    animationFrameRef.current = null;\n                }\n            };\n            recorder.onresume = () => {\n                setIsPaused(false);\n                startTimeRef.current = Date.now();\n                if (enableAnalysis) {\n                    analyzeAudio();\n                }\n            };\n            recorder.onerror = (event) => {\n                setError(`Recording error: ${event.error?.message || \"Unknown error\"}`);\n                setIsRecording(false);\n                setIsPaused(false);\n            };\n            setMediaRecorder(recorder);\n            recorder.start(timeslice);\n            setIsRecording(true);\n            startTimeRef.current = Date.now();\n            pausedTimeRef.current = 0;\n            setDuration(0);\n            intervalRef.current = setInterval(updateDuration, 1000);\n        }\n        catch (err) {\n            const errorMessage = err instanceof Error ? err.message : \"Failed to start recording\";\n            setError(errorMessage);\n        }\n    }, [\n        isSupported,\n        audioBitsPerSecond,\n        mimeType,\n        timeslice,\n        setupAudioAnalysis,\n        updateDuration,\n        enableAnalysis,\n        analyzeAudio,\n    ]);\n    const stopRecording = useCallback(() => {\n        if (mediaRecorder && mediaRecorder.state !== \"inactive\") {\n            mediaRecorder.stop();\n        }\n        if (stream) {\n            stream.getTracks().forEach((track) => track.stop());\n            setStream(null);\n        }\n        if (audioContextRef.current) {\n            audioContextRef.current.close();\n            audioContextRef.current = null;\n        }\n    }, [mediaRecorder, stream]);\n    const pauseRecording = useCallback(() => {\n        if (mediaRecorder && mediaRecorder.state === \"recording\") {\n            mediaRecorder.pause();\n        }\n    }, [mediaRecorder]);\n    const resumeRecording = useCallback(() => {\n        if (mediaRecorder && mediaRecorder.state === \"paused\") {\n            mediaRecorder.resume();\n        }\n    }, [mediaRecorder]);\n    const clearRecording = useCallback(() => {\n        if (audioUrl) {\n            URL.revokeObjectURL(audioUrl);\n        }\n        setAudioBlob(null);\n        setAudioUrl(null);\n        setDuration(0);\n        setAnalysisData(null);\n        setError(null);\n    }, [audioUrl]);\n    const downloadRecording = useCallback((filename = \"recording.webm\") => {\n        if (!audioUrl)\n            return;\n        const link = document.createElement(\"a\");\n        link.href = audioUrl;\n        link.download = filename;\n        document.body.appendChild(link);\n        link.click();\n        document.body.removeChild(link);\n    }, [audioUrl]);\n    useEffect(() => {\n        return () => {\n            if (intervalRef.current) {\n                clearInterval(intervalRef.current);\n            }\n            if (animationFrameRef.current) {\n                cancelAnimationFrame(animationFrameRef.current);\n            }\n            if (stream) {\n                stream.getTracks().forEach((track) => track.stop());\n            }\n            if (audioContextRef.current) {\n                audioContextRef.current.close();\n            }\n            if (audioUrl) {\n                URL.revokeObjectURL(audioUrl);\n            }\n        };\n    }, [stream, audioUrl]);\n    return {\n        isSupported,\n        isRecording,\n        isPaused,\n        stream,\n        mediaRecorder,\n        audioBlob,\n        audioUrl,\n        duration,\n        error,\n        analysisData,\n        startRecording,\n        stopRecording,\n        pauseRecording,\n        resumeRecording,\n        clearRecording,\n        downloadRecording,\n    };\n};"
  }
}